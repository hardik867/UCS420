{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Question 1\n",
        "Write a unique paragraph (5-6 sentences) about your favorite topic (e.g., sports, technology, food, books, etc.).\n",
        "Convert text to lowercase and remove punctuaƟon.\n",
        "Tokenize the text into words and sentences.\n",
        "Remove stopwords (using NLTK's stopwords list).\n",
        "Display word frequency distribuƟon (excluding stopwords)."
      ],
      "metadata": {
        "id": "JtMEcmgWhTqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1gR-olrhULG",
        "outputId": "ca16fb53-1b33-4e2e-fbaf-1c5e659d8a89"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"\"\"Modern life is increasingly shaped by the influence of technology, which is changing how we communicate, work, and handle everyday tasks. From AI-powered tools to smart devices, innovations are becoming part of our daily habits. These changes bring greater convenience and productivity, but they also create new hurdles and possibilities in many fields. Moving ahead, both people and businesses will need to stay flexible and forward-thinking to keep up with the evolving digital landscape.\"\"\""
      ],
      "metadata": {
        "id": "QNHxcBPFhYFd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "text_low = sentence.lower()\n",
        "no_punc = text_low.translate(str.maketrans('', '', string.punctuation))\n",
        "print(no_punc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hx9RhNaciEYs",
        "outputId": "442b3d0e-4e9c-4843-8db3-05c9fb201221"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "modern life is increasingly shaped by the influence of technology which is changing how we communicate work and handle everyday tasks from aipowered tools to smart devices innovations are becoming part of our daily habits these changes bring greater convenience and productivity but they also create new hurdles and possibilities in many fields moving ahead both people and businesses will need to stay flexible and forwardthinking to keep up with the evolving digital landscape\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMg0cBjKi1ou",
        "outputId": "f289ffd2-0274-4b9e-8924-5b6ea14e8899"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hptyameLjKDj",
        "outputId": "e8204263-7b93-495c-b79f-7ac73a007bb2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = word_tokenize(no_punc)\n",
        "sent = sent_tokenize(sentence)\n",
        "print(\"Tokenized words:  \", words,\"\\n\")\n",
        "print(\"Tokenized Sentences:  \", sent,\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbfJOMCzjTvE",
        "outputId": "034aef6b-c08b-47df-ab9c-a9fa9e8b19d1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized words:   ['modern', 'life', 'is', 'increasingly', 'shaped', 'by', 'the', 'influence', 'of', 'technology', 'which', 'is', 'changing', 'how', 'we', 'communicate', 'work', 'and', 'handle', 'everyday', 'tasks', 'from', 'aipowered', 'tools', 'to', 'smart', 'devices', 'innovations', 'are', 'becoming', 'part', 'of', 'our', 'daily', 'habits', 'these', 'changes', 'bring', 'greater', 'convenience', 'and', 'productivity', 'but', 'they', 'also', 'create', 'new', 'hurdles', 'and', 'possibilities', 'in', 'many', 'fields', 'moving', 'ahead', 'both', 'people', 'and', 'businesses', 'will', 'need', 'to', 'stay', 'flexible', 'and', 'forwardthinking', 'to', 'keep', 'up', 'with', 'the', 'evolving', 'digital', 'landscape'] \n",
            "\n",
            "Tokenized Sentences:   ['Modern life is increasingly shaped by the influence of technology, which is changing how we communicate, work, and handle everyday tasks.', 'From AI-powered tools to smart devices, innovations are becoming part of our daily habits.', 'These changes bring greater convenience and productivity, but they also create new hurdles and possibilities in many fields.', 'Moving ahead, both people and businesses will need to stay flexible and forward-thinking to keep up with the evolving digital landscape.'] \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stopper = set(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8PXVEFljWOu",
        "outputId": "4cc2f6db-7e0e-4a60-c323-52d33012ec11"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopper_removed = [w for w in words if w not in stopper]\n",
        "print(\"Filtered Word Tokens:\\n\", stopper_removed ,'\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hH01AN8hjalq",
        "outputId": "fc932555-d10b-47f2-d21f-af8ef742406f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered Word Tokens:\n",
            " ['modern', 'life', 'increasingly', 'shaped', 'influence', 'technology', 'changing', 'communicate', 'work', 'handle', 'everyday', 'tasks', 'aipowered', 'tools', 'smart', 'devices', 'innovations', 'becoming', 'part', 'daily', 'habits', 'changes', 'bring', 'greater', 'convenience', 'productivity', 'also', 'create', 'new', 'hurdles', 'possibilities', 'many', 'fields', 'moving', 'ahead', 'people', 'businesses', 'need', 'stay', 'flexible', 'forwardthinking', 'keep', 'evolving', 'digital', 'landscape'] \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.probability import FreqDist\n",
        "frequency_dist = FreqDist(stopper_removed)\n",
        "print(\"\\nWord Frequency Distribution:\")\n",
        "for word, frequency in frequency_dist.items():\n",
        "    print(f\"{word}: {frequency}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JoLe3LVjdGW",
        "outputId": "084770e1-ee2b-441f-97c0-35b4c1bcf190"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Word Frequency Distribution:\n",
            "modern: 1\n",
            "life: 1\n",
            "increasingly: 1\n",
            "shaped: 1\n",
            "influence: 1\n",
            "technology: 1\n",
            "changing: 1\n",
            "communicate: 1\n",
            "work: 1\n",
            "handle: 1\n",
            "everyday: 1\n",
            "tasks: 1\n",
            "aipowered: 1\n",
            "tools: 1\n",
            "smart: 1\n",
            "devices: 1\n",
            "innovations: 1\n",
            "becoming: 1\n",
            "part: 1\n",
            "daily: 1\n",
            "habits: 1\n",
            "changes: 1\n",
            "bring: 1\n",
            "greater: 1\n",
            "convenience: 1\n",
            "productivity: 1\n",
            "also: 1\n",
            "create: 1\n",
            "new: 1\n",
            "hurdles: 1\n",
            "possibilities: 1\n",
            "many: 1\n",
            "fields: 1\n",
            "moving: 1\n",
            "ahead: 1\n",
            "people: 1\n",
            "businesses: 1\n",
            "need: 1\n",
            "stay: 1\n",
            "flexible: 1\n",
            "forwardthinking: 1\n",
            "keep: 1\n",
            "evolving: 1\n",
            "digital: 1\n",
            "landscape: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2\n",
        "Stemming and Lemmatization\n",
        "Take the tokenized words from QuesƟon 1 (aŌer stopword removal).\n",
        "Apply stemming using NLTK's PorterStemmer and LancasterStemmer.\n",
        "Apply lemmaƟzaƟon using NLTK's WordNetLemmaƟzer.\n",
        "Compare and display results of both techniques."
      ],
      "metadata": {
        "id": "9j498hV4jizl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4') #lematizer\n",
        "porter = PorterStemmer()\n",
        "lancaster = LancasterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "portered = [porter.stem(w) for w in stopper_removed]\n",
        "lancastered = [lancaster.stem(w) for w in stopper_removed]\n",
        "lemmatized = [lemmatizer.lemmatize(w) for w in stopper_removed]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiuPqY1xjfgB",
        "outputId": "be3de90a-7601-40a2-c983-d29129f18c65"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"{'Original':<19} {'Porter':<19} {'Lancaster':<19} {'Lemma':<19}\")\n",
        "print(\"-\" * 60)\n",
        "for o, p, l, le in zip(stopper_removed, portered, lancastered, lemmatized):\n",
        "    print(\"{:<19} {:<19} {:<19} {:<19}\".format(o, p, l, le))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoVCBt7Bjm4Z",
        "outputId": "fd694894-cf41-4917-839f-889f6098c309"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original            Porter              Lancaster           Lemma              \n",
            "------------------------------------------------------------\n",
            "modern              modern              modern              modern             \n",
            "life                life                lif                 life               \n",
            "increasingly        increasingli        increas             increasingly       \n",
            "shaped              shape               shap                shaped             \n",
            "influence           influenc            influ               influence          \n",
            "technology          technolog           technolog           technology         \n",
            "changing            chang               chang               changing           \n",
            "communicate         commun              commun              communicate        \n",
            "work                work                work                work               \n",
            "handle              handl               handl               handle             \n",
            "everyday            everyday            everyday            everyday           \n",
            "tasks               task                task                task               \n",
            "aipowered           aipow               aipow               aipowered          \n",
            "tools               tool                tool                tool               \n",
            "smart               smart               smart               smart              \n",
            "devices             devic               dev                 device             \n",
            "innovations         innov               innov               innovation         \n",
            "becoming            becom               becom               becoming           \n",
            "part                part                part                part               \n",
            "daily               daili               dai                 daily              \n",
            "habits              habit               habit               habit              \n",
            "changes             chang               chang               change             \n",
            "bring               bring               bring               bring              \n",
            "greater             greater             gre                 greater            \n",
            "convenience         conveni             conveny             convenience        \n",
            "productivity        product             produc              productivity       \n",
            "also                also                also                also               \n",
            "create              creat               cre                 create             \n",
            "new                 new                 new                 new                \n",
            "hurdles             hurdl               hurdl               hurdle             \n",
            "possibilities       possibl             poss                possibility        \n",
            "many                mani                many                many               \n",
            "fields              field               field               field              \n",
            "moving              move                mov                 moving             \n",
            "ahead               ahead               ahead               ahead              \n",
            "people              peopl               peopl               people             \n",
            "businesses          busi                busy                business           \n",
            "need                need                nee                 need               \n",
            "stay                stay                stay                stay               \n",
            "flexible            flexibl             flex                flexible           \n",
            "forwardthinking     forwardthink        forwardthink        forwardthinking    \n",
            "keep                keep                keep                keep               \n",
            "evolving            evolv               evolv               evolving           \n",
            "digital             digit               digit               digital            \n",
            "landscape           landscap            landscap            landscape          \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3. Regular Expressions and Text Spliƫng\n",
        "Take their original text from Question 1.\n",
        "\n",
        "Use regular expressions to:\n",
        "\n",
        "a. Extract all words with more than 5 letters.\n",
        "\n",
        "b. Extract all numbers (if any exist in their text).\n",
        "\n",
        "c. Extract all capitalized words.\n",
        "\n",
        "Use text spliƫng techniques to:\n",
        "\n",
        "a. Split the text into words containing only alphabets (removing digits and specialcharacters).\n",
        "b. Extract words starting with a vowel."
      ],
      "metadata": {
        "id": "VreqgcXnj1p8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "letter_5 = re.findall(r'\\b\\w{6,}\\b', no_punc)\n",
        "print(\"words (>5) letters:  \", letter_5 ,'\\n')\n",
        "# \\w{6,} = Match any word (\\w) with 6 or more characters.\n",
        "\n",
        "numb = re.findall(r'\\b\\d+\\b', no_punc)\n",
        "print(\"\\nNumbers :  \", numb,'\\n')\n",
        "# \\d+ = One or more digits (0-9)\n",
        "\n",
        "caps = re.findall(r'\\b[A-Z][a-zA-Z]*\\b', sentence)\n",
        "print(\"\\nCapitalized :  \", caps, '\\n')\n",
        "# [a-zA-Z]* = Followed by zero or more letters (upper/lower case).\n",
        "\n",
        "alphas = re.findall(r'\\b[a-zA-Z]+\\b', no_punc)\n",
        "print(\"\\nOnly alpha words:  \", alphas, '\\n')\n",
        "# [a-zA-Z]+ = One or more alphabet characters (no digits or symbols).\n",
        "\n",
        "# text into words\n",
        "words_list = no_punc.split()\n",
        "only_alpha_words = [w for w in words_list if w.isalpha()]\n",
        "print(\"Only alpha words:\\n\", only_alpha_words)\n",
        "\n",
        "vowelss = [w for w in alphas if w[0].lower() in 'aeiou']\n",
        "print(\"\\nWords starting with vowels:\\n\", vowelss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LhGmofZmjzAy",
        "outputId": "f84281c7-3b54-44a8-acfc-e0db1ba899b7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "words (>5) letters:   ['modern', 'increasingly', 'shaped', 'influence', 'technology', 'changing', 'communicate', 'handle', 'everyday', 'aipowered', 'devices', 'innovations', 'becoming', 'habits', 'changes', 'greater', 'convenience', 'productivity', 'create', 'hurdles', 'possibilities', 'fields', 'moving', 'people', 'businesses', 'flexible', 'forwardthinking', 'evolving', 'digital', 'landscape'] \n",
            "\n",
            "\n",
            "Numbers :   [] \n",
            "\n",
            "\n",
            "Capitalized :   ['Modern', 'From', 'AI', 'These', 'Moving'] \n",
            "\n",
            "\n",
            "Only alpha words:   ['modern', 'life', 'is', 'increasingly', 'shaped', 'by', 'the', 'influence', 'of', 'technology', 'which', 'is', 'changing', 'how', 'we', 'communicate', 'work', 'and', 'handle', 'everyday', 'tasks', 'from', 'aipowered', 'tools', 'to', 'smart', 'devices', 'innovations', 'are', 'becoming', 'part', 'of', 'our', 'daily', 'habits', 'these', 'changes', 'bring', 'greater', 'convenience', 'and', 'productivity', 'but', 'they', 'also', 'create', 'new', 'hurdles', 'and', 'possibilities', 'in', 'many', 'fields', 'moving', 'ahead', 'both', 'people', 'and', 'businesses', 'will', 'need', 'to', 'stay', 'flexible', 'and', 'forwardthinking', 'to', 'keep', 'up', 'with', 'the', 'evolving', 'digital', 'landscape'] \n",
            "\n",
            "Only alpha words:\n",
            " ['modern', 'life', 'is', 'increasingly', 'shaped', 'by', 'the', 'influence', 'of', 'technology', 'which', 'is', 'changing', 'how', 'we', 'communicate', 'work', 'and', 'handle', 'everyday', 'tasks', 'from', 'aipowered', 'tools', 'to', 'smart', 'devices', 'innovations', 'are', 'becoming', 'part', 'of', 'our', 'daily', 'habits', 'these', 'changes', 'bring', 'greater', 'convenience', 'and', 'productivity', 'but', 'they', 'also', 'create', 'new', 'hurdles', 'and', 'possibilities', 'in', 'many', 'fields', 'moving', 'ahead', 'both', 'people', 'and', 'businesses', 'will', 'need', 'to', 'stay', 'flexible', 'and', 'forwardthinking', 'to', 'keep', 'up', 'with', 'the', 'evolving', 'digital', 'landscape']\n",
            "\n",
            "Words starting with vowels:\n",
            " ['is', 'increasingly', 'influence', 'of', 'is', 'and', 'everyday', 'aipowered', 'innovations', 'are', 'of', 'our', 'and', 'also', 'and', 'in', 'ahead', 'and', 'and', 'up', 'evolving']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Q4. Custom Tokenization & Regex-based Text Cleaning\n",
        "Steps:\n",
        "Input: Use the original text from Question 1.\n",
        "\n",
        "Custom Tokenization Function Requirements:\n",
        "\n",
        "(a) Remove punctuation and special symbols, but keep contractions (e.g., \"isn't\" should remain \"isn't\" and not split into \"is\" and \"n't\").\n",
        "\n",
        "(b) Handle hyphenated words as single tokens (e.g., \"state-of-the-art\" should remain as one token).\n",
        "\n",
        "(c) Tokenize numbers separately, but keep decimal numbers intact (e.g., \"3.14\" should remain \"3.14\").\n",
        "\n",
        "Regex Substitutions (using re.sub):\n",
        "\n",
        "(a) Replace email addresses with the placeholder <EMAIL>.\n",
        "\n",
        "(b) Replace URLs with the placeholder <URL>.\n",
        "\n",
        "(c) Replace phone numbers (formats like 123-456-7890 or +91 9876543210) with the plceho-z0-9.-]+.[A-Z|a-z]{2,}\\b placeholder."
      ],
      "metadata": {
        "id": "jEkVv9_VkBAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Tokenizer\n",
        "def custom_tokens(text):\n",
        "    text = re.sub(r\"[^a-zA-Z0-9\\s\\-']\", \"\", text)\n",
        "    tokens = re.findall(r\"\\b\\w+(?:[-']\\w+)*\\b\", text)\n",
        "    # ?: = Non-capturing group (just groups the pattern, doesn't save it separately)\n",
        "    return tokens\n",
        "customs = custom_tokens(sentence)\n",
        "print(\"\\nCustom Tokens:  \", customs, '\\n')\n",
        "\n",
        "\n",
        "text_replaced = re.sub(r'\\b[\\w.-]+?@\\w+?\\.\\w+?\\b', '<EMAIL>', sentence)\n",
        "# [\\w.-]+?\tOne or more word characters (a-z, A-Z, 0-9, _), dot ., or hyphen - (email username part)\n",
        "# \\w+?\tDomain name (like gmail) and then the domain extension\n",
        "\n",
        "text_replaced = re.sub(r'http[s]?://\\S+', '<URL>', text_replaced)\n",
        "# http[s]?\tMatch http or https (the s? means \"s\" is optional)\n",
        "# \\S+\tMatch one or more non-space characters (the URL itself)\n",
        "\n",
        "text_replaced = re.sub(r'(\\+?\\d{1,2}\\s?)?(\\d{3}[-\\s]?\\d{3}[-\\s]?\\d{4})', '<PHONE>', text_replaced)\n",
        "# (\\+?\\d{1,2}\\s?)? – The entire pattern is optional due to the outer ()\n",
        "# \\+?  Matches an optional plus sign (+)\n",
        "# \\d{1,2}  Matches 1 or 2 digits (0-9)\n",
        "# \\s?   Matches an optional whitespace character (like a space or tab)\n",
        "# [-\\s]?   Optional hyphen or space (separator)\n",
        "# \\d{4}\tExactly 4 digits (last part)\n",
        "print(\"\\nText after Regex Substitutions:\\n\", text_replaced)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0k4oQjxj80y",
        "outputId": "eb848c8a-7050-4269-f199-c3e187e2965c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Custom Tokens:   ['Modern', 'life', 'is', 'increasingly', 'shaped', 'by', 'the', 'influence', 'of', 'technology', 'which', 'is', 'changing', 'how', 'we', 'communicate', 'work', 'and', 'handle', 'everyday', 'tasks', 'From', 'AI-powered', 'tools', 'to', 'smart', 'devices', 'innovations', 'are', 'becoming', 'part', 'of', 'our', 'daily', 'habits', 'These', 'changes', 'bring', 'greater', 'convenience', 'and', 'productivity', 'but', 'they', 'also', 'create', 'new', 'hurdles', 'and', 'possibilities', 'in', 'many', 'fields', 'Moving', 'ahead', 'both', 'people', 'and', 'businesses', 'will', 'need', 'to', 'stay', 'flexible', 'and', 'forward-thinking', 'to', 'keep', 'up', 'with', 'the', 'evolving', 'digital', 'landscape'] \n",
            "\n",
            "\n",
            "Text after Regex Substitutions:\n",
            " Modern life is increasingly shaped by the influence of technology, which is changing how we communicate, work, and handle everyday tasks. From AI-powered tools to smart devices, innovations are becoming part of our daily habits. These changes bring greater convenience and productivity, but they also create new hurdles and possibilities in many fields. Moving ahead, both people and businesses will need to stay flexible and forward-thinking to keep up with the evolving digital landscape.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KHl79R3gkFdi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}